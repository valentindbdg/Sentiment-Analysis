{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Sentiment Analysis using GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d068f5a82b76bcf6b3eca451cf471011522f8da6"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ccfb0cd343e60360991427828430077e096c705"
   },
   "source": [
    "# 1) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bbcc4eec6e7af0dffdf891472c242cf8425b831"
   },
   "source": [
    "## 1.1) Import and load the datasets (train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Valentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import talos as ta\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from talos.model.normalizers import lr_normalizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#%matplotlib inline\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba0fa0ea411eb91fbc69f77c2dbeccc3dcbe0d30"
   },
   "source": [
    "## 1.2) Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71400354b6a77193ebf2c8b2203e848ac5734aad"
   },
   "source": [
    "Read in the data and create a pandas dataframe of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3bbcbc2c6294a63bc68ede7437712ccd2ceabee3"
   },
   "outputs": [],
   "source": [
    "#Cov = pd.read_csv(\"path/to/file.txt\", sep='\\t', \n",
    "#                  names = [\"Text, \"Label])\n",
    "#Frame=pd.DataFrame([Cov], columns = [\"Text\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "878a28af2fcedcbfd3802f4afc19de1304b71322"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/sentimentanalysis/imdb_labelled.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14060/3932381050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Opening and Reading the files into a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/sentimentanalysis/imdb_labelled.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Read the lines from both the files and append in same list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/sentimentanalysis/yelp_labelled.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/sentimentanalysis/imdb_labelled.txt'"
     ]
    }
   ],
   "source": [
    "# Opening and Reading the files into a list\n",
    "with open(\"../input/sentimentanalysis/imdb_labelled.txt\",\"r\") as text_file:\n",
    "    lines = text_file.read().split('\\n')\n",
    "# Read the lines from both the files and append in same list\n",
    "with open(\"../input/sentimentanalysis/yelp_labelled.txt\",\"r\") as text_file:\n",
    "    lines += text_file.read().split('\\n')\n",
    "with open(\"../input/sentimentanalysis/amazon_cells_labelled.txt\",\"r\") as text_file:\n",
    "    lines += text_file.read().split('\\n')\n",
    "\n",
    "# split by tab and remove corrupted data if any or lines which are not tab seperated\n",
    "lines = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\n",
    "train_documents = [line[0] for line in lines ]\n",
    "train_labels = [int(line[1]) for line in lines]\n",
    "\n",
    "# Now we have split the sentences and the labels in two lists of the same order. Every data refers two one row.\n",
    "data_full = [[train_documents[i], train_labels[i]] for i in range(len(train_documents))]\n",
    "train_text = [train_documents[i] for i in range(len(train_documents))]\n",
    "train_label = [train_labels[i] for i in range(len(train_documents))]\n",
    "from numpy import array\n",
    "train_text = array(train_text)\n",
    "train_label = array(train_label)\n",
    "\n",
    "df = pd.DataFrame(data_full)\n",
    "df_text = pd.DataFrame(train_text)\n",
    "df_label = pd.DataFrame(train_label)\n",
    "train_documents = array(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a16415fbd4ac4784c9ef9af0f4a2a66a22c5d9ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_label)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95c0af304e21f772cc5800a5a2a2cafb95475f80"
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(train_documents, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc088c2beb65a753da1269f2ec2f057dde2c22cc"
   },
   "outputs": [],
   "source": [
    "print (xtrain, type(xtrain), xtrain.shape, type(xtrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b23fe84e7d2239ad91454a3fd5ed815a7b7893bf"
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(xtrain, ytrain, \n",
    "                                                  stratify=ytrain, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71ea5ae77bd954c14bddae32bde153f850e438da"
   },
   "source": [
    "Word2vec produces one vector per word, whereas tf-idf produces a score. Word2vec is great for going deeper into the documents we have and helps in identifying content and subsets of content. Its vectors represent each wordâ€™s context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b742d1c93b7c8ba566df1b4a30ad86f134b0f36b"
   },
   "outputs": [],
   "source": [
    "print (xtrain, type(xtrain), xtrain.shape, type(xtrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d00e226276e3eb8fab396af9c9c99a3106eb60e3"
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "ytest_enc = np_utils.to_categorical(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8de69db2f6c09c90eb6254d8bf3114ff92a3892"
   },
   "outputs": [],
   "source": [
    "print(ytrain_enc.shape, ytrain_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "051beb203c80f90a5a65b0f5577380df68d9ad70",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove42/glove.42B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf7c7b5239f2a3894a7b6f74e8f639ce6cd2c93d"
   },
   "source": [
    "This part is used for GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba3cd79f22b50b93ce664e1b5aecbbd8551d5189"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid) + list(xtest))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "xtest_seq = token.texts_to_sequences(xtest)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0c87ca7c289c0547c906fbb6cdf6a136457b9f0"
   },
   "outputs": [],
   "source": [
    "print(xtrain_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9e2cabb4fb6649d8b02d993f7b071cdff5956f1"
   },
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fe05dbb0443087feff109c3f574be9a8825a140"
   },
   "source": [
    "## 1.3) Build the model, compile and set the parameters for grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da595bb2148b76985d073d81fcd1ff156b142459"
   },
   "source": [
    "For this network (and GRU as well), I used Keras because I never used it before and I wanted to try it. Besides, I also chose to use Tanos, a custom package to do grid search, and find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbc40e903a3652596142f753c49ad06299197d1d"
   },
   "outputs": [],
   "source": [
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from talos.model.normalizers import lr_normalizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(xtrain_pad, ytrain_enc, xvalid_pad, yvalid_enc, params):\n",
    "    model = Sequential()                            \n",
    "    #Dense1:\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                        300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "    \n",
    "    #Dense(2):\n",
    "    model.add(Dense(params['second_neuron'], activation='relu'))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    #Dense(3):\n",
    "    model.add(Dense(params['third_neuron'], activation='relu'))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    #Dense(4):\n",
    "    model.add(Dense(ytrain_enc.shape[1], \n",
    "                    activation=params['last_activation']))\n",
    "    #Compile:\n",
    "    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "                  loss=params['loss'],\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    out = model.fit(xtrain_pad, ytrain_enc,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=1,\n",
    "                    validation_data=[xvalid_pad, yvalid_enc],\n",
    "                    callbacks=early_stopper(params['epochs'], patience=3, mode='moderate', monitor='val_loss'))\n",
    "    \n",
    "    return out, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f0b693c845c45685244a4556c1a4f190cb60607"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.activations import softmax\n",
    "from keras.losses import categorical_crossentropy, logcosh\n",
    "p = {'lr': (0.1, 10, 10),\n",
    "     'second_neuron': [800, 900, 1000, 1100, 1200, 1500],\n",
    "     'third_neuron': [800, 900, 1000, 1100, 1200],\n",
    "     'batch_size': [2000],\n",
    "     'epochs': [100],\n",
    "     'dropout': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "     'optimizer': [Adam],\n",
    "     'loss': ['categorical_crossentropy'],\n",
    "     'last_activation': ['softmax'],\n",
    "     'weight_regulizer': [None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "feab0af9eaa2a38bb7e893733d015c62696b1920",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = ta.Scan(xtrain_pad, ytrain_enc, params=p, model=create_model, grid_downsample=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29472ec5c66c338d6480c0f93a29ecbeaef6d2f7"
   },
   "source": [
    "# 2) Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "222231507b705a769caf4874f685530b5d2a6808"
   },
   "outputs": [],
   "source": [
    "h.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bef57753013034528cb9ae0b4ba9c1e0df4ef60e"
   },
   "outputs": [],
   "source": [
    "h.peak_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40db4d5efd65a29d08ad99e4a6fba616f7872a19"
   },
   "outputs": [],
   "source": [
    "# access the summary details\n",
    "h.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f31a9c917a99cc24f2e0fac72f9881b7cd71b250"
   },
   "outputs": [],
   "source": [
    "e = ta.Evaluate(h)\n",
    "e.evaluate(xtest_pad, ytest_enc, folds=2, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79e0c79c58cf4d760ad3c0bbc40309ddfa3c7352"
   },
   "outputs": [],
   "source": [
    "# use Scan object as input\n",
    "r = ta.Reporting(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a7ef7086819493849c4d878385971216e042d8c"
   },
   "outputs": [],
   "source": [
    "# get the highest result ('val_acc' by default)\n",
    "r.high('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c75d07e83c6048b425426f789f8c517174c2ba87"
   },
   "outputs": [],
   "source": [
    "# get the highest result ('val_acc' by default)\n",
    "r.high('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ad561d25ae191bec1a2ecda626c1d39d9937ffd"
   },
   "outputs": [],
   "source": [
    "# get the round with the best result\n",
    "r.rounds2high()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "483426939f987d71aa70d70216a2205411a91a27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the best paramaters\n",
    "r.best_params('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b1e8e741cdb605ca6629cd318574127e926ebaa"
   },
   "outputs": [],
   "source": [
    "# get the best paramaters\n",
    "r.best_params('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d3031a5e94f6e79945ad00ecbc2fa9009d42a8a"
   },
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(len(word_index) + 1,\n",
    "#                     300,\n",
    "#                     weights=[embedding_matrix],\n",
    "#                     input_length=max_len,\n",
    "#                     trainable=False))\n",
    "#model.add(SpatialDropout1D(0.3))\n",
    "#model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "#model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "#\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "#model.add(Dense(2))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "#model.fit(xtrain_pad, y=ytrain_enc, batch_size=1000, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b9601cbd181998c80d78fd12b896c1b0e6bf6ed"
   },
   "outputs": [],
   "source": [
    "#e = model.evaluate(x=xtest_pad, y=ytest_enc, batch_size=1000, verbose=1, sample_weight=None, steps=None)\n",
    "#print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1702934cd6d12b0196f6c1e9dc930087212d07d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
