{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Sentiment Analysis : GRU model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"# Introduction","metadata":{"_uuid":"d068f5a82b76bcf6b3eca451cf471011522f8da6"}},{"cell_type":"markdown","source":"WHY GRU?\n\nGRUs have been shown to exhibit better performance on smaller datasets.  \n\n(Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". arXiv:1412.3555 [cs.NE].)\n","metadata":{"_uuid":"d83e1a080892f6f2c0aaf58a35ad3d3ab5429119"}},{"cell_type":"markdown","source":"## 1) Method","metadata":{"_uuid":"6ccfb0cd343e60360991427828430077e096c705"}},{"cell_type":"markdown","source":"## 1.1) Import and load the datasets (train+test)","metadata":{"_uuid":"9bbcc4eec6e7af0dffdf891472c242cf8425b831"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\n#from keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.layers import BatchNormalization\n\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nimport nltk\nnltk.download('punkt')\n!pip install talos\nimport talos as ta\nfrom talos.model.early_stopper import early_stopper\nfrom talos.model.normalizers import lr_normalizer\nfrom keras.wrappers.scikit_learn import KerasClassifier \nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n#%matplotlib inline\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None\n\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-03-10T12:21:33.581071Z","iopub.execute_input":"2022-03-10T12:21:33.581562Z","iopub.status.idle":"2022-03-10T12:22:23.172859Z","shell.execute_reply.started":"2022-03-10T12:21:33.581472Z","shell.execute_reply":"2022-03-10T12:22:23.171804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2) Preprocessing\n","metadata":{"_uuid":"ba0fa0ea411eb91fbc69f77c2dbeccc3dcbe0d30","trusted":true}},{"cell_type":"markdown","source":"Read in the data and create a pandas dataframe of it","metadata":{"_uuid":"71400354b6a77193ebf2c8b2203e848ac5734aad"}},{"cell_type":"code","source":"#Cov = pd.read_csv(\"path/to/file.txt\", sep='\\t', \n#                  names = [\"Text, \"Label])\n#Frame=pd.DataFrame([Cov], columns = [\"Text\", \"Label\"])","metadata":{"_uuid":"3bbcbc2c6294a63bc68ede7437712ccd2ceabee3","execution":{"iopub.status.busy":"2022-03-10T12:22:23.175391Z","iopub.execute_input":"2022-03-10T12:22:23.175656Z","iopub.status.idle":"2022-03-10T12:22:23.183169Z","shell.execute_reply.started":"2022-03-10T12:22:23.175625Z","shell.execute_reply":"2022-03-10T12:22:23.181951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Opening and Reading the files into a list\nwith open(\"../input/sentimentanalysis/imdb_labelled.txt\",\"r\") as text_file:\n    lines = text_file.read().split('\\n')\n# Read the lines from both the files and append in same list\nwith open(\"../input/sentimentanalysis/yelp_labelled.txt\",\"r\") as text_file:\n    lines += text_file.read().split('\\n')\nwith open(\"../input/sentimentanalysis/amazon_cells_labelled.txt\",\"r\") as text_file:\n    lines += text_file.read().split('\\n')\n\n# split by tab and remove corrupted data if any or lines which are not tab seperated\nlines = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']\ntrain_documents = [line[0] for line in lines ]\ntrain_labels = [int(line[1]) for line in lines]\n\n# Now we have split the sentences and the labels in two lists of the same order. Every data refers two one row.\ndata_full = [[train_documents[i], train_labels[i]] for i in range(len(train_documents))]\ntrain_text = [train_documents[i] for i in range(len(train_documents))]\ntrain_label = [train_labels[i] for i in range(len(train_documents))]\nfrom numpy import array\ntrain_text = array(train_text)\ntrain_label = array(train_label)\n\ndf = pd.DataFrame(data_full)\ndf_text = pd.DataFrame(train_text)\ndf_label = pd.DataFrame(train_label)\ntrain_documents = array(train_documents)","metadata":{"_uuid":"878a28af2fcedcbfd3802f4afc19de1304b71322","execution":{"iopub.status.busy":"2022-03-10T12:22:23.184958Z","iopub.execute_input":"2022-03-10T12:22:23.1856Z","iopub.status.idle":"2022-03-10T12:22:23.239067Z","shell.execute_reply.started":"2022-03-10T12:22:23.185564Z","shell.execute_reply":"2022-03-10T12:22:23.238072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(df_label)\ntype(y)","metadata":{"_uuid":"a16415fbd4ac4784c9ef9af0f4a2a66a22c5d9ec","scrolled":true,"execution":{"iopub.status.busy":"2022-03-10T12:22:23.241624Z","iopub.execute_input":"2022-03-10T12:22:23.241957Z","iopub.status.idle":"2022-03-10T12:22:23.255141Z","shell.execute_reply.started":"2022-03-10T12:22:23.241914Z","shell.execute_reply":"2022-03-10T12:22:23.254157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(train_documents, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)\n","metadata":{"_uuid":"95c0af304e21f772cc5800a5a2a2cafb95475f80","execution":{"iopub.status.busy":"2022-03-10T12:22:23.25708Z","iopub.execute_input":"2022-03-10T12:22:23.25763Z","iopub.status.idle":"2022-03-10T12:22:23.272738Z","shell.execute_reply.started":"2022-03-10T12:22:23.257368Z","shell.execute_reply":"2022-03-10T12:22:23.271813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (xtrain, type(xtrain), xtrain.shape, type(xtrain[0]))","metadata":{"_uuid":"fc088c2beb65a753da1269f2ec2f057dde2c22cc","execution":{"iopub.status.busy":"2022-03-10T12:22:23.274525Z","iopub.execute_input":"2022-03-10T12:22:23.274908Z","iopub.status.idle":"2022-03-10T12:22:23.282638Z","shell.execute_reply.started":"2022-03-10T12:22:23.274843Z","shell.execute_reply":"2022-03-10T12:22:23.281591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(xtrain, ytrain, \n                                                  stratify=ytrain, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","metadata":{"_uuid":"b23fe84e7d2239ad91454a3fd5ed815a7b7893bf","execution":{"iopub.status.busy":"2022-03-10T12:22:23.284856Z","iopub.execute_input":"2022-03-10T12:22:23.285872Z","iopub.status.idle":"2022-03-10T12:22:23.30086Z","shell.execute_reply.started":"2022-03-10T12:22:23.285825Z","shell.execute_reply":"2022-03-10T12:22:23.299836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word2vec produces one vector per word, whereas tf-idf produces a score. Word2vec is great for going deeper into the documents we have and helps in identifying content and subsets of content. Its vectors represent each wordâ€™s context.","metadata":{"_uuid":"71ea5ae77bd954c14bddae32bde153f850e438da","trusted":true}},{"cell_type":"code","source":"print (xtrain, type(xtrain), xtrain.shape, type(xtrain[0]))","metadata":{"_uuid":"b742d1c93b7c8ba566df1b4a30ad86f134b0f36b","execution":{"iopub.status.busy":"2022-03-10T12:22:23.302443Z","iopub.execute_input":"2022-03-10T12:22:23.304472Z","iopub.status.idle":"2022-03-10T12:22:23.312468Z","shell.execute_reply.started":"2022-03-10T12:22:23.304428Z","shell.execute_reply":"2022-03-10T12:22:23.310944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)\nytest_enc = np_utils.to_categorical(ytest)\n","metadata":{"_uuid":"d00e226276e3eb8fab396af9c9c99a3106eb60e3","execution":{"iopub.status.busy":"2022-03-10T12:22:23.314664Z","iopub.execute_input":"2022-03-10T12:22:23.314966Z","iopub.status.idle":"2022-03-10T12:22:23.32286Z","shell.execute_reply.started":"2022-03-10T12:22:23.314937Z","shell.execute_reply":"2022-03-10T12:22:23.32165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ytrain_enc.shape, ytrain_enc)","metadata":{"_uuid":"b8de69db2f6c09c90eb6254d8bf3114ff92a3892","execution":{"iopub.status.busy":"2022-03-10T12:22:23.327883Z","iopub.execute_input":"2022-03-10T12:22:23.32859Z","iopub.status.idle":"2022-03-10T12:22:23.336217Z","shell.execute_reply.started":"2022-03-10T12:22:23.328539Z","shell.execute_reply":"2022-03-10T12:22:23.335068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('../input/glove42/glove.42B.300d.txt', encoding=\"utf8\")\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"_uuid":"051beb203c80f90a5a65b0f5577380df68d9ad70","execution":{"iopub.status.busy":"2022-03-10T12:22:23.338188Z","iopub.execute_input":"2022-03-10T12:22:23.338863Z","iopub.status.idle":"2022-03-10T12:25:49.168165Z","shell.execute_reply.started":"2022-03-10T12:22:23.338817Z","shell.execute_reply":"2022-03-10T12:25:49.167177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This part is used for GRU model","metadata":{"_uuid":"cf7c7b5239f2a3894a7b6f74e8f639ce6cd2c93d","trusted":true}},{"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid) + list(xtest))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\nxtest_seq = token.texts_to_sequences(xtest)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\nxtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n\n\nword_index = token.word_index","metadata":{"_uuid":"ba3cd79f22b50b93ce664e1b5aecbbd8551d5189","execution":{"iopub.status.busy":"2022-03-10T12:25:49.16962Z","iopub.execute_input":"2022-03-10T12:25:49.170576Z","iopub.status.idle":"2022-03-10T12:25:49.319141Z","shell.execute_reply.started":"2022-03-10T12:25:49.17053Z","shell.execute_reply":"2022-03-10T12:25:49.318113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xtrain_pad.shape)","metadata":{"_uuid":"d0c87ca7c289c0547c906fbb6cdf6a136457b9f0","execution":{"iopub.status.busy":"2022-03-10T12:25:49.320788Z","iopub.execute_input":"2022-03-10T12:25:49.321347Z","iopub.status.idle":"2022-03-10T12:25:49.327699Z","shell.execute_reply.started":"2022-03-10T12:25:49.321301Z","shell.execute_reply":"2022-03-10T12:25:49.326562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"_uuid":"e9e2cabb4fb6649d8b02d993f7b071cdff5956f1","execution":{"iopub.status.busy":"2022-03-10T12:25:49.329247Z","iopub.execute_input":"2022-03-10T12:25:49.331336Z","iopub.status.idle":"2022-03-10T12:25:49.372434Z","shell.execute_reply.started":"2022-03-10T12:25:49.331302Z","shell.execute_reply":"2022-03-10T12:25:49.370147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3) Build the model, compile and set the parameters for grid search","metadata":{"_uuid":"7fe05dbb0443087feff109c3f574be9a8825a140"}},{"cell_type":"markdown","source":"For this network (and GRU as well), I used Keras because I never used it before and I wanted to try it. Besides, I also chose to use Tanos, a custom package to do grid search, and find the best parameters.","metadata":{"_uuid":"da595bb2148b76985d073d81fcd1ff156b142459"}},{"cell_type":"code","source":"import talos as ta\nfrom talos.model.early_stopper import early_stopper\nfrom talos.model.normalizers import lr_normalizer\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ndef create_model(xtrain_pad, ytrain_enc, xvalid_pad, yvalid_enc, params):\n    model = Sequential()                            \n    #Dense1:\n    model.add(Embedding(len(word_index) + 1,\n                        300,\n                        weights=[embedding_matrix],\n                        input_length=max_len,\n                        trainable=False))\n    model.add(SpatialDropout1D(0.3))\n    model.add(GRU(params['first_neuron'], dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n    model.add(GRU(params['first_neuron'], dropout=0.3, recurrent_dropout=0.3))\n    \n    #Dense(2):\n    model.add(Dense(params['second_neuron'], activation='relu'))\n    model.add(Dropout(params['dropout']))\n    \n    #Dense(3):\n    model.add(Dense(params['third_neuron'], activation='relu'))\n    model.add(Dropout(params['dropout']))\n    \n    #Dense(4):\n    model.add(Dense(ytrain_enc.shape[1], \n                    activation=params['last_activation']))\n    #Compile:\n    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n                  loss=params['loss'],\n                  metrics=['acc'])\n\n    out = model.fit(xtrain_pad, ytrain_enc,\n                    batch_size=params['batch_size'],\n                    epochs=params['epochs'],\n                    verbose=1,\n                    validation_data=[xvalid_pad, yvalid_enc],\n                    callbacks=early_stopper(params['epochs'], patience=3, mode='moderate', monitor='val_loss'))\n    \n    return out, model\n\n\n","metadata":{"_uuid":"fbc40e903a3652596142f753c49ad06299197d1d","execution":{"iopub.status.busy":"2022-03-10T12:25:49.374096Z","iopub.execute_input":"2022-03-10T12:25:49.374957Z","iopub.status.idle":"2022-03-10T12:25:49.388347Z","shell.execute_reply.started":"2022-03-10T12:25:49.374909Z","shell.execute_reply":"2022-03-10T12:25:49.387107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from keras.optimizers import Adam, Nadam\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.activations import softmax\nfrom keras.losses import categorical_crossentropy, logcosh\np = {'lr': (0.1, 10, 10),\n     'first_neuron': [100, 300, 400, 500, 600],\n     'second_neuron': [800, 900, 1000, 1100, 1200, 1500],\n     'third_neuron': [800, 900, 1000, 1100, 1200],\n     'batch_size': [2000],\n     'epochs': [100],\n     'dropout': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n     'optimizer': [Adam],\n     'loss': ['categorical_crossentropy'],\n     'last_activation': ['softmax'],\n     'weight_regulizer': [None]}","metadata":{"_uuid":"9f0b693c845c45685244a4556c1a4f190cb60607","execution":{"iopub.status.busy":"2022-03-10T12:29:36.213593Z","iopub.execute_input":"2022-03-10T12:29:36.213989Z","iopub.status.idle":"2022-03-10T12:29:36.221596Z","shell.execute_reply.started":"2022-03-10T12:29:36.21394Z","shell.execute_reply":"2022-03-10T12:29:36.220627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = ta.Scan(xtrain_pad, ytrain_enc, params=p, model=create_model, fraction_limit=0.01, random_method='quantum', experiment_name=\"test\")","metadata":{"_uuid":"feab0af9eaa2a38bb7e893733d015c62696b1920","scrolled":true,"execution":{"iopub.status.busy":"2022-03-10T12:33:03.094057Z","iopub.execute_input":"2022-03-10T12:33:03.094393Z","iopub.status.idle":"2022-03-10T13:08:41.692597Z","shell.execute_reply.started":"2022-03-10T12:33:03.094359Z","shell.execute_reply":"2022-03-10T13:08:41.69053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Result","metadata":{"_uuid":"29472ec5c66c338d6480c0f93a29ecbeaef6d2f7"}},{"cell_type":"code","source":"h.data","metadata":{"_uuid":"222231507b705a769caf4874f685530b5d2a6808","execution":{"iopub.status.busy":"2022-03-10T13:08:55.710997Z","iopub.execute_input":"2022-03-10T13:08:55.712569Z","iopub.status.idle":"2022-03-10T13:08:55.796293Z","shell.execute_reply.started":"2022-03-10T13:08:55.712519Z","shell.execute_reply":"2022-03-10T13:08:55.788135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h.peak_epochs_df","metadata":{"_uuid":"bef57753013034528cb9ae0b4ba9c1e0df4ef60e","execution":{"iopub.status.busy":"2022-03-10T13:08:59.428588Z","iopub.execute_input":"2022-03-10T13:08:59.428934Z","iopub.status.idle":"2022-03-10T13:08:59.462026Z","shell.execute_reply.started":"2022-03-10T13:08:59.428872Z","shell.execute_reply":"2022-03-10T13:08:59.460929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# access the summary details\nh.details","metadata":{"_uuid":"40db4d5efd65a29d08ad99e4a6fba616f7872a19","execution":{"iopub.status.busy":"2022-03-10T13:09:25.63942Z","iopub.execute_input":"2022-03-10T13:09:25.639721Z","iopub.status.idle":"2022-03-10T13:09:25.650661Z","shell.execute_reply.started":"2022-03-10T13:09:25.639686Z","shell.execute_reply":"2022-03-10T13:09:25.648801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = ta.Evaluate(h)\ne.evaluate(xtest_pad, ytest_enc, task='multi_label', metric='val_acc')","metadata":{"_uuid":"f31a9c917a99cc24f2e0fac72f9881b7cd71b250","execution":{"iopub.status.busy":"2022-03-10T13:18:28.159051Z","iopub.execute_input":"2022-03-10T13:18:28.15956Z","iopub.status.idle":"2022-03-10T13:18:31.841173Z","shell.execute_reply.started":"2022-03-10T13:18:28.159522Z","shell.execute_reply":"2022-03-10T13:18:31.84005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use Scan object as input\nr = ta.Reporting(h)","metadata":{"_uuid":"79e0c79c58cf4d760ad3c0bbc40309ddfa3c7352","execution":{"iopub.status.busy":"2022-03-10T13:18:36.565338Z","iopub.execute_input":"2022-03-10T13:18:36.565995Z","iopub.status.idle":"2022-03-10T13:18:36.572235Z","shell.execute_reply.started":"2022-03-10T13:18:36.565959Z","shell.execute_reply":"2022-03-10T13:18:36.570877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the highest result ('val_acc' by default)\nr.high('val_acc')","metadata":{"_uuid":"1a7ef7086819493849c4d878385971216e042d8c","execution":{"iopub.status.busy":"2022-03-10T13:18:37.590434Z","iopub.execute_input":"2022-03-10T13:18:37.591103Z","iopub.status.idle":"2022-03-10T13:18:37.599224Z","shell.execute_reply.started":"2022-03-10T13:18:37.591066Z","shell.execute_reply":"2022-03-10T13:18:37.597786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the highest result ('val_acc' by default)\nr.high('acc')","metadata":{"_uuid":"c75d07e83c6048b425426f789f8c517174c2ba87","execution":{"iopub.status.busy":"2022-03-10T13:18:38.845722Z","iopub.execute_input":"2022-03-10T13:18:38.846218Z","iopub.status.idle":"2022-03-10T13:18:38.855738Z","shell.execute_reply.started":"2022-03-10T13:18:38.846183Z","shell.execute_reply":"2022-03-10T13:18:38.85461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the round with the best result\nr.rounds2high()","metadata":{"_uuid":"9ad561d25ae191bec1a2ecda626c1d39d9937ffd","execution":{"iopub.status.busy":"2022-03-10T13:18:42.195621Z","iopub.execute_input":"2022-03-10T13:18:42.195966Z","iopub.status.idle":"2022-03-10T13:18:42.223019Z","shell.execute_reply.started":"2022-03-10T13:18:42.195926Z","shell.execute_reply":"2022-03-10T13:18:42.221597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the best paramaters\nr.best_params('acc')","metadata":{"_uuid":"483426939f987d71aa70d70216a2205411a91a27","scrolled":true,"execution":{"iopub.status.busy":"2022-03-10T13:18:43.417752Z","iopub.execute_input":"2022-03-10T13:18:43.418635Z","iopub.status.idle":"2022-03-10T13:18:43.443967Z","shell.execute_reply.started":"2022-03-10T13:18:43.418597Z","shell.execute_reply":"2022-03-10T13:18:43.442365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the best paramaters\nr.best_params('val_acc')","metadata":{"_uuid":"4b1e8e741cdb605ca6629cd318574127e926ebaa","execution":{"iopub.status.busy":"2022-03-10T13:18:44.705806Z","iopub.execute_input":"2022-03-10T13:18:44.706539Z","iopub.status.idle":"2022-03-10T13:18:44.753098Z","shell.execute_reply.started":"2022-03-10T13:18:44.706478Z","shell.execute_reply":"2022-03-10T13:18:44.750194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"852a68459154a5a4a976f51c40aee8e4fb0a4d7d"}},{"cell_type":"code","source":"","metadata":{"_uuid":"375505c1f7bf7af0f684b6f45c57e680a19443a0","trusted":true},"execution_count":null,"outputs":[]}]}
